#configuration file
[acoustic_network_params]
num_layers : 2
hidden_size : 768
# Dropout value during training
# any value between 0.5 and 0.9 should be fine, 0.5 seems to be the best value
dropout : 0.5
batch_size : 50
learning_rate : 1e-3
lr_decay_factor : 0.90
grad_clip : 5

[lm_network_params]
num_layers : 3
hidden_size : 34
dropout : 0.9
batch_size : 1
learning_rate : 1e-5
lr_decay_factor : 0.97
grad_clip : 5

[general]
# Whether to read config settings if pre-existing ones are found in checkpoint path
use_config_file_if_checkpoint_exists : True
steps_per_checkpoint : 100
checkpoint_dir : data/checkpoints/

[training]
# Training dataset parameters
# Supported training dataset types are : LibriSpeech, Shtooka, Vystadial_2013 or TEDLIUM
training_dataset_dir : data/Shtooka/train
training_dataset_type : Shtooka
# Test dataset parameters (optional)
# Supported training dataset types are : LibriSpeech, Shtooka, Vystadial_2013 or TEDLIUM
test_dataset_dir : data/Shtooka/test
test_dataset_type : Shtooka
# Fraction of the training set used for test set (optional)
# If test and training dataset are not separated then a fraction of the training set can be used for test
# Should not be provided if test_dataset_dir and test_dataset_type are provided
#train_frac : 0.98

# max_input_seq_length is the maximum number of 0.01s chunks we allow in a single training (or test) example
#   Recommended : 2000 chuncks (for 20 seconds) but it require more than 8 Gb of memory
# max_target_seq_length is the maximum number of letters we allow in a single training (or test) example output
#   Must be less than 65535 (int16 for reduced memory consumption)
#   Recommended : 400 characters (for 20 seconds seems reasonnable in most languages)
# Warnings :
#  - Those two values need to be consistent one with the other
#  - These values depends on your dataset used for training
# Small config (8 Gb memory) :
#  - use 800 / 250 respectively
#  - remove wav files over this limit from the dataset
#     (CAREFULL recursive delete : find -name *.wav -size +250k -exec rm {} \;)
# Shtooka training dataset : 350 / 40 should be enough
# Vystadial_2013 training dataset : 900 / 100 should be ok
# TEDLIUM training dataset : 1200 / 300 should be ok (small config : 900 / 300 with batch_size 8)
max_input_seq_length : 350
max_target_seq_length : 40
# Load input_vec from ".h5" file over ".wav" file if available, save ".h5" file for future use otherwise
# Faster learning over timer but it will take some space on the disk
load_save_input_vec = True
# Asynchronous loading of the data for the batch
# Warning : async_get_batch and load_save_input_vec cannot be true at the same time
async_get_batch = False
# Filter files that are too long before starting training
# This will exclude files before starting to train (more efficient) but it require to process the input vector for
# each file before starting to train which can be time consumming.
# It is strongly recommended to use "load_save_input_vec = True" if you intend to set this to True
pre_filter_file_size = True
# Create a tensorboard file during training (directory or blank, directory must already exist, won't be created)
# Launch tensorboard in another terminal with : "tensorboard --logdir=data/tensorboard/"
tensorboard_dir = data/tensorboard
