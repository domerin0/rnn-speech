#configuration file
[acoustic_network_params]
num_layers : 2
hidden_size : 300
dropout : 0.7
batch_size : 50
train_frac : 0.98
learning_rate : 1e-5
lr_decay_factor : 0.97
grad_clip : 5

[lm_network_params]
num_layers : 3
hidden_size : 34
dropout : 0.7
batch_size : 1
train_frac : 0.7
learning_rate : 1e-5
lr_decay_factor : 0.97
grad_clip : 5

[general]
# Whether to read config settings if pre-existing ones are found in checkpoint path
use_config_file_if_checkpoint_exists : True
steps_per_checkpoint : 100
checkpoint_dir : data/checkpoints/

[training]
training_dataset_dir : data/LibriSpeech/
# max_input_seq_length is the maximum number of 0.01s chunks we allow in a single training (or test) example
#   Recommended : 2000 chuncks (for 20 seconds) but it require more than 8 Gb of memory
# max_target_seq_length is the maximum number of letters we allow in a single training (or test) example output
#   Must be less than 65535 (int16 for reduced memory consumption)
#   Recommended : 400 characters (for 20 seconds seems reasonnable in most languages)
# Warnings :
#  - Those two values need to be consistent one with the other
#  - These values depends on your dataset used for training
# Small config (8 Gb memory) :
#  - use 800 / 250 respectively
#  - remove wav files over this limit from the dataset
#     (CAREFULL recursive delete : find -name *.wav -size +250k -exec rm {} \;)
max_input_seq_length : 800
max_target_seq_length : 250
# Load input_vec from .lmfb file over wav file if available, and save lmfb file otherwise
# Faster learning over timer but it will take some space on the disk
load_save_input_vec = True
