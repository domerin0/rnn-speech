#configuration file
[acoustic_network_params]
num_layers : 2
hidden_size : 768
# Dropout value during training
# Those values define the probability to keep a cell active during training
# Recommanded values are 0.8 for input (dropping 20%) and 0.5 for output (dropping 50%)
# http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
dropout_input_keep_prob : 0.8
dropout_output_keep_prob : 0.5
# Batch size should be between 2 and 50
# You should find the greater value which allow your graphic card time to be
# "optimized" (check that you GPU never go to 0% usage for example)
# 10 is a good value if your GPU card can afford
batch_size : 10
learning_rate : 0.001
lr_decay_factor : 0.10
grad_clip : 5
#options are mfcc, and fbank
#mfcc is Mel Frequency Cepstral Coefficients (20-dim input)
#fbank is mel filterbank (including delta and double-delta, 120-dim input)
#mfcc is essentially fbank with an extra DCT (discrete cosine transformation) applied
#delta and double-delta represent 'velocity' and 'acceleration' of the input signal
signal_processing : mfcc

[lm_network_params]
num_layers : 3
hidden_size : 34
dropout : 0.9
batch_size : 1
learning_rate : 1e-5
lr_decay_factor : 0.97
grad_clip : 5

[general]
# Whether to read config settings if pre-existing ones are found in checkpoint path
use_config_file_if_checkpoint_exists : True
steps_per_checkpoint : 1000
checkpoint_dir : data/checkpoints/

[training]
# Note : supported datasets are LibriSpeech, Shtooka, Vystadial_2013 or TEDLIUM
# Training dataset dirs (comma separated)
training_dataset_dirs : data/Shtooka/train, data/LibriSpeech/train
# Test dataset dirs (optional, comma separated)
test_dataset_dirs : data/LibriSpeech/test
# Fraction of the training set used for test set (optional)
# If test and training datasets are not separated then a fraction of the training set can be used for test
# Should not be provided if test_dataset_dirs is provided
#train_frac : 0.98

# max_input_seq_length is the maximum number of 0.01s chunks we allow in a single training (or test) example
# max_target_seq_length is the maximum number of letters we allow in a single training (or test) example output
#   Must be less than 65535 (int16 for reduced memory consumption)
# Advices :
#  - Those two values need to be consistent one with the other
#  - These values depends on your dataset used for training
#  - For reduced memory consumption it is possible to reduce batch_size
# LibriSpeech training dataset : 1510 / 580 with batch_size 10 (small config : 800 / 300 with batch_size 2)
# Shtooka training dataset : 350 / 40 should be enough
# Vystadial_2013 training dataset : 900 / 100 should be ok
# TEDLIUM training dataset : 1200 / 300 should be ok (small config : 900 / 300 with batch_size 8)
max_input_seq_length : 1510
max_target_seq_length : 580
# Create a tensorboard file during training (directory or blank, directory must already exist, won't be created)
# Launch tensorboard in another terminal with : "tensorboard --logdir=data/tensorboard/"
tensorboard_dir : data/tensorboard
# Apply batch normalization to the data during training (True / False)
batch_normalization : False

[logging]
# Set a log file, if void then log messages will be outputed to the screen
log_file :
# Set a log level : DEBUG, INFO, WARNING (default) , ERROR or CRITICAL
log_level : INFO
